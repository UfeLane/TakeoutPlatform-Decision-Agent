import pandas as pd
import json
import os
from core import Agent
from personas import AGENTS_CONFIG


# ================= 1. æ•°æ®åŠ è½½ä¸æŠ½æ ·å‡½æ•° =================
def load_sampled_data(csv_filename, samples_per_class=5):
    """
    è¯»å– CSVï¼Œåªå– 'content' å’Œ 'cluster_label' ä¸¤åˆ—
    ä»æ¯ä¸ªç±»åˆ«ä¸­éšæœºæŠ½å– 5 æ¡ï¼Œå¹¶æŒ‰ç±»åˆ«é¡ºåºæ’åˆ—ã€‚
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œç¡®ä¿ä¸€å®šèƒ½æ‰¾åˆ°æ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(current_dir, csv_filename)

    print(f"ğŸ“‚ æ­£åœ¨å°è¯•è¯»å–æ–‡ä»¶: {csv_path}")

    try:
        # 1. è¯»å– CSV (å°è¯•ä¸åŒç¼–ç é˜²æ­¢ä¹±ç )
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            print("âš ï¸ UTF-8 è¯»å–å¤±è´¥ï¼Œå°è¯• GBK ç¼–ç ...")
            df = pd.read_csv(csv_path, encoding='gbk')

        # 2. æ£€æŸ¥åˆ—åæ˜¯å¦æ­£ç¡®
        # ä½ æˆªå›¾é‡Œçš„åˆ—åæ˜¯ 'content' å’Œ 'cluster_label'
        required_cols = ['content', 'cluster_label']
        if not set(required_cols).issubset(df.columns):
            print(f"âŒ åˆ—åé”™è¯¯ï¼ä½ çš„CSVå¿…é¡»åŒ…å«: {required_cols}")
            print(f"   ä½ å½“å‰çš„åˆ—åæ˜¯: {df.columns.tolist()}")
            return []

        # 3. åªä¿ç•™éœ€è¦çš„ä¸¤åˆ—ï¼Œå»é™¤ç©ºå€¼
        df = df[required_cols].dropna()

        # 4. åˆ†å±‚æŠ½æ · (Stratified Sampling)
        # ä»æ¯ä¸ª cluster_label é‡ŒæŠ½ 5 æ¡
        # å¦‚æœæŸç±»ä¸è¶³ 5 æ¡ï¼Œå°±å…¨éƒ¨å–å‡ºæ¥
        sampled_df = df.groupby('cluster_label', group_keys=False).apply(
            lambda x: x.sample(min(len(x), samples_per_class), random_state=42)
        )

        # 5. ã€å…³é”®ã€‘æŒ‰ cluster_label æ’åº
        # è¿™æ ·å–‚ç»™æ™ºèƒ½ä½“æ—¶ï¼Œè¯„è®ºæ˜¯æŒ‰è¯é¢˜åˆ†å—çš„ï¼ˆæ¯”å¦‚å…ˆå…¨æ˜¯ä»·æ ¼ç±»ï¼Œå†å…¨æ˜¯æœåŠ¡ç±»...ï¼‰
        # è¿™ä¼šè®©æŠ˜çº¿å›¾å‡ºç°æ¼‚äº®çš„â€œé˜¶æ®µæ€§æ³¢åŠ¨â€
        sampled_df = sampled_df.sort_values('cluster_label')

        print(f"âœ… æˆåŠŸæŠ½å–æ•°æ®ï¼šå…± {len(sampled_df)} æ¡")
        print(f"   åŒ…å«è¯é¢˜ç±»åˆ«: {sampled_df['cluster_label'].unique()}")

        # åªè¿”å›è¯„è®ºå†…å®¹çš„åˆ—è¡¨ç»™æ™ºèƒ½ä½“
        return sampled_df['content'].tolist()

    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ï¼è¯·ç¡®è®¤æ–‡ä»¶åæ˜¯å¦ä¸º: {csv_filename}")
        return []
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []


# ================= 2. ä»¿çœŸä¸»é€»è¾‘ =================
def run_simulation(comment_list):
    """
    æ¥æ”¶è¯„è®ºåˆ—è¡¨ï¼Œè®©æ‰€æœ‰æ™ºèƒ½ä½“é˜…è¯»å¹¶ååº”
    """
    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    agents = [Agent(cfg["name"], cfg["desc"]) for cfg in AGENTS_CONFIG]

    full_logs = []
    line_chart_data = {a.name: [] for a in agents}

    print(f"\nğŸš€ å¼€å§‹ä»¿çœŸï¼š{len(agents)} ä¸ªæ™ºèƒ½ä½“ x {len(comment_list)} æ¡è¯„è®º")
    print("-" * 50)

    # å¾ªç¯äº¤äº’
    for i, comment in enumerate(comment_list):
        # ç®€å•çš„è¿›åº¦å±•ç¤ºï¼Œé¿å…åˆ·å±
        print(f"[{i + 1}/{len(comment_list)}] æ­£åœ¨é˜…è¯»: {comment[:20]}...")

        for agent in agents:
            # è°ƒç”¨æ ¸å¿ƒæ¨¡å— (core.py)
            res = agent.perceive(comment)
            if res:
                full_logs.append(res)
                line_chart_data[agent.name].append(res['cumulative_score'])

    return {
        "raw_logs": full_logs,  # è¯¦ç»†æ—¥å¿— (ç»™çƒ­åŠ›å›¾/æ¡‘åŸºå›¾)
        "line_chart": line_chart_data  # è¶‹åŠ¿æ•°æ® (ç»™æŠ˜çº¿å›¾)
    }


# ================= 3. ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    # è¯·ç¡®ä¿è¿™ä¸ªåå­—å’Œä½ æˆªå›¾é‡Œçš„å®Œå…¨ä¸€æ ·
    CSV_FILENAME = "allcomments_with_label_processed.csv"

    # 1. åŠ è½½å¹¶æŠ½æ ·æ•°æ®
    comments_input = load_sampled_data(CSV_FILENAME, samples_per_class=5)

    # 2. å¦‚æœæœ‰æ•°æ®ï¼Œå¼€å§‹ä»¿çœŸ
    if comments_input:
        result = run_simulation(comments_input)

        # 3. æ‰“å°ç¬¬ä¸€æ¡ç»“æœéªŒè¯
        if result["raw_logs"]:
            print("\nâœ… ä»¿çœŸå®Œæˆï¼è¾“å‡ºç¤ºä¾‹ (ç¬¬ä¸€æ¡æ—¥å¿—):")
            print(json.dumps(result["raw_logs"][0], indent=4, ensure_ascii=False))

            # (å¯é€‰) ä¿å­˜ç»“æœæ–‡ä»¶ï¼Œæ–¹ä¾¿ä»¥ååˆ†ææˆ–ç»™å‰ç«¯ç”¨
            with open("simulation_result.json", "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=4)
            print("\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: simulation_result.json")
    else:
        print("\nâš ï¸ æ— æ³•è·å–è¯„è®ºæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")